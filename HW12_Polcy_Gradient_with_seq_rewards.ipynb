{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw12_Polcy_Gradient_with_seq_rewards_try_number_two_version.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fp30SB4bxeQb"
      },
      "source": [
        "# **Homework 12 - Reinforcement Learning**\n",
        "\n",
        "若有任何問題，歡迎來信至助教信箱 ntu-ml-2021spring-ta@googlegroups.com\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaixTDcoUuLB"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mpZnF5vVaiz"
      },
      "source": [
        "from google.colab import drive # Import a library named google.colab\n",
        "drive.mount('/content/drive', force_remount=True) # mount the content to the directory `/content/drive`\n",
        "\n",
        "%cd /content/drive/MyDrive/ML2021/HW\n",
        "# !mkdir HW12     # I HAVE MADE IT.\n",
        "%cd HW12"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXsnCWPtWSNk"
      },
      "source": [
        "## 前置作業\n",
        "\n",
        "首先我們需要安裝必要的系統套件及 PyPi 套件。\n",
        "gym 這個套件由 OpenAI 所提供，是一套用來開發與比較 Reinforcement Learning 演算法的工具包（toolkit）。\n",
        "而其餘套件則是為了在 Notebook 中繪圖所需要的套件。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5e2bScpnkVbv"
      },
      "source": [
        "!apt update\n",
        "!apt install python-opengl xvfb -y\n",
        "!pip install gym[box2d]==0.18.3 pyvirtualdisplay tqdm numpy==1.19.5 torch==1.8.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_-i3cdoYsks"
      },
      "source": [
        "接下來，設置好 virtual display，並引入所有必要的套件。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nl2nREINDLiw"
      },
      "source": [
        "%%capture\n",
        "from pyvirtualdisplay import Display\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVu9-Vdrl4E3"
      },
      "source": [
        "# 請不要更改 random seed !!!!\n",
        "# 不然在judgeboi上 你的成績不會被reproduce !!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fV9i8i2YkRbO"
      },
      "source": [
        "seed = 543 # Do not change this\n",
        "def fix(env, seed):\n",
        "  env.seed(seed)\n",
        "  env.action_space.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  np.random.seed(seed)\n",
        "  random.seed(seed)\n",
        "  torch.set_deterministic(True)\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "  torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He0XDx6bzjgC"
      },
      "source": [
        "最後，引入 OpenAI 的 gym，並建立一個 [Lunar Lander](https://gym.openai.com/envs/LunarLander-v2/) 環境。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_4-xJcbBt09"
      },
      "source": [
        "%%capture\n",
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "env = gym.make('LunarLander-v2')\n",
        "\n",
        "fix(env, seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmiAOfqRwRX5"
      },
      "source": [
        "import time\n",
        "start = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcMjEUWTBEEB"
      },
      "source": [
        "!pip freeze"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrkVvTrvWZ5H"
      },
      "source": [
        "## 什麼是 Lunar Lander？\n",
        "\n",
        "“LunarLander-v2” 這個環境是在模擬登月小艇降落在月球表面時的情形。\n",
        "這個任務的目標是讓登月小艇「安全地」降落在兩個黃色旗幟間的平地上。\n",
        "> Landing pad is always at coordinates (0,0).\n",
        "> Coordinates are the first two numbers in state vector.\n",
        "\n",
        "![](https://gym.openai.com/assets/docs/aeloop-138c89d44114492fd02822303e6b4b07213010bb14ca5856d2d49d6b62d88e53.svg)\n",
        "\n",
        "所謂的「環境」其實同時包括了 agent 和 environment。\n",
        "我們利用 `step()` 這個函式讓 agent 行動，而後函式便會回傳 environment 給予的 observation/state（以下這兩個名詞代表同樣的意思）和 reward。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIbp82sljvAt"
      },
      "source": [
        "### Observation / State\n",
        "\n",
        "首先，我們可以看看 environment 回傳給 agent 的 observation 究竟是長什麼樣子的資料："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsXZra3N9R5T"
      },
      "source": [
        "print(env.observation_space)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezdfoThbAQ49"
      },
      "source": [
        "`Box(8,)` 說明我們會拿到 8 維的向量作為 observation，其中包含：垂直及水平座標、速度、角度、加速度等等，這部分我們就不細說。\n",
        "\n",
        "### Action\n",
        "\n",
        "而在 agent 得到 observation 和 reward 以後，能夠採取的動作有："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1k4dIrBAaKi"
      },
      "source": [
        "print(env.action_space)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dejXT6PHBrPn"
      },
      "source": [
        "`Discrete(4)` 說明 agent 可以採取四種離散的行動：\n",
        "- 0 代表不採取任何行動\n",
        "- 2 代表主引擎向下噴射\n",
        "- 1, 3 則是向左右噴射\n",
        "\n",
        "接下來，我們嘗試讓 agent 與 environment 互動。\n",
        "在進行任何操作前，建議先呼叫 `reset()` 函式讓整個「環境」重置。\n",
        "而這個函式同時會回傳「環境」最初始的狀態。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pi4OmrmZgnWA"
      },
      "source": [
        "initial_state = env.reset()\n",
        "print(initial_state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBx0mEqqgxJ9"
      },
      "source": [
        "接著，我們試著從 agent 的四種行動空間中，隨機採取一個行動"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxkOEXRKgizt"
      },
      "source": [
        "random_action = env.action_space.sample()\n",
        "print(random_action)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mns-bO01g0-J"
      },
      "source": [
        "再利用 `step()` 函式讓 agent 根據我們隨機抽樣出來的 `random_action` 動作。\n",
        "而這個函式會回傳四項資訊：\n",
        "- observation / state\n",
        "- reward\n",
        "- 完成與否\n",
        "- 其餘資訊"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_WViSxGgIk9"
      },
      "source": [
        "observation, reward, done, info = env.step(random_action)\n",
        "# print(reward)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdieGq7NuBIm"
      },
      "source": [
        "第一項資訊 `observation` 即為 agent 採取行動之後，agent 對於環境的 observation 或者說環境的 state 為何。\n",
        "而第三項資訊 `done` 則是 `True` 或 `False` 的布林值，當登月小艇成功著陸或是不幸墜毀時，代表這個回合（episode）也就跟著結束了，此時 `step()` 函式便會回傳 `done = True`，而在那之前，`done` 則保持 `False`。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yK7r126kuCNp"
      },
      "source": [
        "print(done)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKdS8vOihxhc"
      },
      "source": [
        "### Reward\n",
        "\n",
        "而「環境」給予的 reward 大致是這樣計算：\n",
        "- 小艇墜毀得到 -100 分\n",
        "- 小艇在黃旗幟之間成功著地則得 100~140 分\n",
        "- 噴射主引擎（向下噴火）每次 -0.3 分\n",
        "- 小艇最終完全靜止則再得 100 分\n",
        "- 小艇每隻腳碰觸地面 +10 分\n",
        "\n",
        "> Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points.\n",
        "> If lander moves away from landing pad it loses reward back.\n",
        "> Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points.\n",
        "> Each leg ground contact is +10.\n",
        "> Firing main engine is -0.3 points each frame.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxQNs77hi0_7"
      },
      "source": [
        "print(reward) # after doing a random action (0), the immediate reward is stored in this "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mhqp6D-XgHpe"
      },
      "source": [
        "### Random Agent\n",
        "\n",
        "最後，在進入實做之前，我們就來看看這樣一個 random agent 能否成功登陸月球："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3G0bxoccelv"
      },
      "source": [
        "\n",
        "env.reset()\n",
        "\n",
        "img = plt.imshow(env.render(mode='rgb_array'))\n",
        "\n",
        "done = False\n",
        "while not done:\n",
        "    action = env.action_space.sample()\n",
        "    observation, reward, done, _ = env.step(action)\n",
        "\n",
        "    img.set_data(env.render(mode='rgb_array'))\n",
        "    display.display(plt.gcf())\n",
        "    display.clear_output(wait=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5paWqo7tWL2"
      },
      "source": [
        "## Policy Gradient\n",
        "\n",
        "現在來搭建一個簡單的 policy network。\n",
        "我們預設模型的輸入是 8-dim 的 observation，輸出則是離散的四個動作之一："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8tdmeD-tZew"
      },
      "source": [
        "class PolicyGradientNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(8, 16)\n",
        "        self.fc2 = nn.Linear(16, 16)\n",
        "        self.fc3 = nn.Linear(16, 4)\n",
        "\n",
        "    def forward(self, state):\n",
        "        hid = torch.tanh(self.fc1(state))\n",
        "        hid = torch.tanh(self.fc2(hid))\n",
        "        return F.softmax(self.fc3(hid), dim=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynbqJrhIFTC3"
      },
      "source": [
        "再來，搭建一個簡單的 agent，並搭配上方的 policy network 來採取行動。\n",
        "這個 agent 能做到以下幾件事：\n",
        "- `learn()`：從記下來的 log probabilities 及 rewards 來更新 policy network。\n",
        "- `sample()`：從 environment 得到 observation 之後，利用 policy network 得出應該採取的行動。\n",
        "而此函式除了回傳抽樣出來的 action，也會回傳此次抽樣的 log probabilities。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZo-IxJx286z"
      },
      "source": [
        "\n",
        "class PolicyGradientAgent():\n",
        "    \n",
        "    # Here is the place you intialize policy parameters\n",
        "    def __init__(self, network):\n",
        "        self.network = network\n",
        "        self.optimizer = optim.SGDSS(self.network.parameters(), lr=0.0007) # the default is 0.001\n",
        "         \n",
        "    def forward(self, state):\n",
        "        return self.network(state)\n",
        "\n",
        "    # Here is the place you Calculate discounted reward and improve your parameters\n",
        "    # rewards is a list rightnow\n",
        "    def learn(self, log_probs, rewards):\n",
        "        loss = (-log_probs * rewards).sum() # You don't need to revise this to pass simple baseline (but you can)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        \n",
        "    def sample(self, state):\n",
        "        action_prob = self.network(torch.FloatTensor(state))\n",
        "        action_dist = Categorical(action_prob)\n",
        "        action = action_dist.sample()\n",
        "        log_prob = action_dist.log_prob(action)\n",
        "        return action.item(), log_prob\n",
        "\n",
        "    def save(self, PATH): # You should not revise this\n",
        "        Agent_Dict = {\n",
        "            \"network\" : self.network.state_dict(),\n",
        "            \"optimizer\" : self.optimizer.state_dict()\n",
        "        }\n",
        "        torch.save(Agent_Dict, PATH)\n",
        "\n",
        "    def load(self, PATH): # You should not revise this\n",
        "        checkpoint = torch.load(PATH)\n",
        "        self.network.load_state_dict(checkpoint[\"network\"])\n",
        "        #如果要儲存過程或是中斷訓練後想繼續可以用喔 ^_^\n",
        "        self.optimizer.load_state_dict(checkpoint[\"optimizer\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehPlnTKyRZf9"
      },
      "source": [
        "最後，建立一個 network 和 agent，就可以開始進行訓練了。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1gd0_J0OUWa"
      },
      "source": [
        "# For the model save and load"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-1ZyyG_OXQZ"
      },
      "source": [
        "#　https://pytorch.org/tutorials/beginner/saving_loading_models.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfJIvML-RYjL"
      },
      "source": [
        "# Here is the palce we initialze policy parameters (THETA)\n",
        "\n",
        "network = PolicyGradientNetwork() # THE PLACE FOR RETRAINING\n",
        "agent = PolicyGradientAgent(network) # THE PLACE FOR RETRAINING\n",
        "#np.load(PATH,allow_pickle=True)\n",
        "# ckpt = np.load('policygradientsecondversion_3.npy',allow_pickle=True)\n",
        "# agent.load((\"PG_4.ckpt\")) # i did it ! \n",
        "# agent.load('PG_1.ckpt',allow_pickle=True)　#load the ckpt file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RL34NnItUM7k"
      },
      "source": [
        "# 參考資料_Poicy_Gradient\n",
        "## https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-R4EQlWhWcR"
      },
      "source": [
        "# GITHUB CODE 支援"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvdsdh54hZbp"
      },
      "source": [
        "  '''\n",
        "  https://github.com/keon/policy-gradient/blob/master/pg.py\n",
        "  The code about discount_rewards\n",
        "  discounted_rewards = np.zeros_like(rewards)\n",
        "  running_add = 0\n",
        "  for t in reversed(range(0, rewards.size)):\n",
        "      if rewards[t] != 0:\n",
        "      running_add = 0\n",
        "      running_add = running_add * self.gamma + rewards[t]\n",
        "      discounted_rewards[t] = running_add\n",
        "  return discounted_rewards\n",
        "  '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouv23glgf5Qt"
      },
      "source": [
        "## 訓練 Agent\n",
        "\n",
        "現在我們開始訓練 agent。\n",
        "透過讓 agent 和 environment 互動，我們記住每一組對應的 log probabilities 及 reward，並在成功登陸或者不幸墜毀後，回放這些「記憶」來訓練 policy network。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vg5rxBBaf38_"
      },
      "source": [
        "agent.network.train()  # 訓練前，先確保 network 處在 training 模式\n",
        "EPISODE_PER_BATCH = 5  # 每蒐集 5 個 episodes 更新一次 agent\n",
        "NUM_BATCH = 4000        # 總共更新 400 次\n",
        "\n",
        "avg_total_rewards, avg_final_rewards = [], []\n",
        "\n",
        "prg_bar = tqdm(range(NUM_BATCH))\n",
        "\n",
        "# new number\n",
        "gamma = 0.9 # we can try it to 0.95 for gamma later\n",
        "for batch in prg_bar:\n",
        "\n",
        "    # log_probs, rewards = [], []\n",
        "    rewards = []\n",
        "    # total_rewards, final_rewards = [], []\n",
        "\n",
        "    # 蒐集訓練資料\n",
        "    for episode in range(EPISODE_PER_BATCH):\n",
        "        \n",
        "        state = env.reset() \n",
        "        # total_reward, total_step = 0, 0\n",
        "        seq_rewards = [] # maybe here is the place for trajectory\n",
        "        log_probs = [] # for the test to record the logs and implement the policy gradient\n",
        "        while True:\n",
        "\n",
        "            action, log_prob = agent.sample(state) # at , log(at|st)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            log_probs.append(log_prob) # [log(a1|s1), log(a2|s2), ...., log(at|st)]\n",
        "\n",
        "            seq_rewards.append(reward) # for the every case.\n",
        "            state = next_state\n",
        "            # total_reward += reward\n",
        "            # total_step += 1\n",
        "           \n",
        "            # rewards.append(reward) #改這裡\n",
        "\n",
        "            discounted_rewards = np.zeros_like(seq_rewards)\n",
        "            running_add = 0\n",
        "            for t in reversed(range(0, len(seq_rewards))):\n",
        "              #  print(\"for check \" + str(len(rewards)) , t)\n",
        "               if seq_rewards[t] != 0:\n",
        "                  running_add = 0\n",
        "               running_add = running_add * gamma + seq_rewards[t]\n",
        "               discounted_rewards[t] = running_add\n",
        "            # rewards = discounted_rewards\n",
        "            for t in range(len(seq_rewards)):\n",
        "              seq_rewards[t] = discounted_rewards[t]\n",
        "            \n",
        "            # print(\"The reward \", reward)\n",
        "            # print(\"Rewards Length \", len(rewards)) \n",
        "            # ! 重要 ！\n",
        "            # for i in range(total_step-1):\n",
        "            #     rewards[i] += reward*(0.99**(i+1)) # for the accumulative decaying reward\n",
        "\n",
        "            # 現在的reward 的implementation 為每個時刻的瞬時reward, 給定action_list : a1, a2, a3 ......\n",
        "            #                                                       reward :     r1, r2 ,r3 ......\n",
        "            # medium：將reward調整成accumulative decaying reward, 給定action_list : a1,                         a2,                           a3 ......\n",
        "            #                                                       reward :     r1+0.99*r2+0.99^2*r3+......, r2+0.99*r3+0.99^2*r4+...... ,r3+0.99*r4+0.99^2*r5+ ......\n",
        "            # boss : implement DQN\n",
        "            if done:\n",
        "                # for t in range(0, len(seq_rewards)):\n",
        "                #     rewards.append(seq_rewards[t])\n",
        "\n",
        "                # final_rewards.append(reward)\n",
        "                # total_rewards.append(total_reward)\n",
        "                seq_rewards = (seq_rewards - np.mean(seq_rewards)) / (np.std(seq_rewards) + 1e-9)  # 將 reward 正規標準化\n",
        "                agent.learn(torch.stack(log_probs), torch.from_numpy(seq_rewards))\n",
        "\n",
        "                break\n",
        "\n",
        "    # The below uncomment for just watching the result.\n",
        "    # print(f\"rewards looks like \", np.shape(rewards))  \n",
        "    # print(f\"log_probs looks like \", np.shape(log_probs))     \n",
        "    # # 紀錄訓練過程\n",
        "    # avg_total_reward = sum(total_rewards) / len(total_rewards)\n",
        "    # avg_final_reward = sum(final_rewards) / len(final_rewards)\n",
        "    # avg_total_rewards.append(avg_total_reward)\n",
        "    # avg_final_rewards.append(avg_final_reward)\n",
        "    # prg_bar.set_description(f\"Total: {avg_total_reward: 4.1f}, Fina\n",
        "    \n",
        "    # l: {avg_final_reward: 4.1f}\")\n",
        "\n",
        "    # 更新網路\n",
        "    # rewards = np.concatenate(rewards, axis=0)\n",
        "    # rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-9)  # 將 reward 正規標準化\n",
        "    # agent.learn(torch.stack(log_probs), torch.from_numpy(rewards))\n",
        "    # print(\"logs prob looks like \", torch.stack(log_probs).size())\n",
        "    # print(\"torch.from_numpy(rewards) looks like \", torch.from_numpy(rewards).size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNb_tuFYhKVK"
      },
      "source": [
        "### 訓練結果\n",
        "\n",
        "訓練過程中，我們持續記下了 `avg_total_reward`，這個數值代表的是：每次更新 policy network 前，我們讓 agent 玩數個回合（episodes），而這些回合的平均 total rewards 為何。\n",
        "理論上，若是 agent 一直在進步，則所得到的 `avg_total_reward` 也會持續上升，直至 250 上下。\n",
        "若將其畫出來則結果如下："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZYOI8H10SHN"
      },
      "source": [
        "end = time.time()\n",
        "plt.plot(avg_total_rewards)\n",
        "plt.title(\"Total Rewards\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV5jj4dThz0Y"
      },
      "source": [
        "另外，`avg_final_reward` 代表的是多個回合的平均 final rewards，而 final reward 即是 agent 在單一回合中拿到的最後一個 reward。\n",
        "如果同學們還記得環境給予登月小艇 reward 的方式，便會知道，不論**回合的最後**小艇是不幸墜毀、飛出畫面、或是靜止在地面上，都會受到額外地獎勵或處罰。\n",
        "也因此，final reward 可被用來觀察 agent 的「著地」是否順利等資訊。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txYwqoU9p57z"
      },
      "source": [
        "# 你需要uncomment掉為了優化所不跑的code(紀錄訓練過程)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txDZ5vlGWz5w"
      },
      "source": [
        "plt.plot(avg_final_rewards)\n",
        "plt.title(\"Final Rewards\")\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyT7tNwkVdS-"
      },
      "source": [
        "訓練時間\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_t-JsKxUViFy"
      },
      "source": [
        "print(f\"total time is {end-start} sec\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2HaGRVEYGQS"
      },
      "source": [
        "## 測試"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yFuUKKRYH73"
      },
      "source": [
        "fix(env, seed)\n",
        "agent.network.eval()  # 測試前先將 network 切換為 evaluation 模式\n",
        "NUM_OF_TEST = 5 # Do not revise it !!!!!\n",
        "test_total_reward = []\n",
        "action_list = []\n",
        "for i in range(NUM_OF_TEST):\n",
        "  actions = []\n",
        "  state = env.reset()\n",
        "\n",
        "  img = plt.imshow(env.render(mode='rgb_array'))\n",
        "\n",
        "  total_reward = 0\n",
        "\n",
        "  done = False\n",
        "  while not done:\n",
        "      action, _ = agent.sample(state)\n",
        "      actions.append(action)\n",
        "      state, reward, done, _ = env.step(action)\n",
        "\n",
        "      total_reward += reward\n",
        "\n",
        "      #img.set_data(env.render(mode='rgb_array'))\n",
        "      #display.display(plt.gcf())\n",
        "      #display.clear_output(wait=True)\n",
        "  print(total_reward)\n",
        "  test_total_reward.append(total_reward)\n",
        "\n",
        "  action_list.append(actions) #儲存你測試的結果\n",
        "  print(\"length of actions is \", len(actions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aex7mcKr0J01"
      },
      "source": [
        "print(f\"Your final reward is : %.2f\"%np.mean(test_total_reward))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leyebGYRpqsF"
      },
      "source": [
        "Action list 的長相"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGAH4YWDpp4u"
      },
      "source": [
        "print(\"Action list looks like \", action_list)\n",
        "print(\"Action list's shape looks like \", np.shape(action_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7sokqEUtrFY"
      },
      "source": [
        "Action 的分布\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHdAItjj1nxw"
      },
      "source": [
        "distribution = {}\n",
        "for actions in action_list:\n",
        "  for action in actions:\n",
        "    if action not in distribution.keys():\n",
        "      distribution[action] = 1\n",
        "    else:\n",
        "      distribution[action] += 1\n",
        "print(distribution)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ricE0schY75M"
      },
      "source": [
        "儲存 Model Testing的結果\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZsMkGmIY42b"
      },
      "source": [
        "PATH = \"PG_Adam.npy\"  # 可以改成你想取的名字或路徑\n",
        "agent.save(\"PG_Adam.ckpt\")\n",
        "np.save(PATH ,np.array(action_list)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asK7WfbkaLjt"
      },
      "source": [
        "### 你要交到JudgeBoi的檔案94這個\n",
        "儲存結果到本地端 (就是你的電腦裡拉 = = )\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-CqyhHzaWAL"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seT4NUmWmAZ1"
      },
      "source": [
        "# Server 測試\n",
        "到時候下面會是我們Server上測試的環境，可以給大家看一下自己的表現如何"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOPmW8o0P78q"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U69c-YTxaw6b"
      },
      "source": [
        "\n",
        "action_list = np.load('PG_4.npy',allow_pickle=True) #到時候你上傳的檔案.\n",
        "\n",
        "# action_list = np.load('Policy_Gradient_Third_Try_Help_From_Github.npy', \n",
        "# allow_pickle=True) #嘗試從電腦上放測試檔案\n",
        "\n",
        "seed = 543 #到時候測試的seed 請不要更改\n",
        "fix(env, seed)\n",
        "\n",
        "agent.network.eval()  # 測試前先將 network 切換為 evaluation 模式\n",
        "\n",
        "'''\n",
        "env.reset()\n",
        "\n",
        "img = plt.imshow(env.render(mode='rgb_array'))\n",
        "\n",
        "done = False\n",
        "while not done:\n",
        "    action = env.action_space.sample()\n",
        "    observation, reward, done, _ = env.step(action)\n",
        "\n",
        "    img.set_data(env.render(mode='rgb_array'))\n",
        "    display.display(plt.gcf())\n",
        "    display.clear_output(wait=True)\n",
        "'''\n",
        "test_total_reward = []\n",
        "for actions in action_list:\n",
        "  state = env.reset()\n",
        "  img = plt.imshow(env.render(mode='rgb_array'))\n",
        "\n",
        "  total_reward = 0\n",
        "\n",
        "  done = False\n",
        "  # while not done:\n",
        "  done_count = 0\n",
        "  for action in actions:\n",
        "      # action, _ = agent1.sample(state)\n",
        "      state, reward, done, _ = env.step(action)\n",
        "      # img.set_data(env.render(mode='rgb_array'))\n",
        "      # display.display(plt.gcf())\n",
        "      # display.clear_output(wait=True)\n",
        "      done_count += 1\n",
        "      total_reward += reward\n",
        "      if done:\n",
        "        \n",
        "        break\n",
        "    #   img.set_data(env.render(mode='rgb_array'))\n",
        "    #   display.display(plt.gcf())\n",
        "    #   display.clear_output(wait=True)\n",
        "  print(f\"Your reward is : %.2f\"%total_reward)\n",
        "  test_total_reward.append(total_reward)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjFBWwQP1hVe"
      },
      "source": [
        "# 你的成績"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpJpZz3Wbm0X"
      },
      "source": [
        "print(f\"Your final reward is : %.2f\"%np.mean(test_total_reward))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUBtYXG2eaqf"
      },
      "source": [
        "## 參考資料\n",
        "\n",
        "以下是一些有用的參考資料。\n",
        "建議同學們實做前，可以先參考第一則連結的上課影片。\n",
        "在影片的最後有提到兩個有用的 Tips，這對於本次作業的實做非常有幫助。\n",
        "\n",
        "- [DRL Lecture 1: Policy Gradient (Review)](https://youtu.be/z95ZYgPgXOY)\n",
        "- [ML Lecture 23-3: Reinforcement Learning (including Q-learning) start at 30:00](https://youtu.be/2-JNBzCq77c?t=1800)\n",
        "- [Lecture 7: Policy Gradient, David Silver](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/pg.pdf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGqP2EU1joWM"
      },
      "source": [
        ""
      ]
    }
  ]
}